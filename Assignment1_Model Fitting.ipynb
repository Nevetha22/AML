{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf873b83",
   "metadata": {},
   "source": [
    "#### Nevetha N G\n",
    "#### MDS202128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e359f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as met\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2e0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv(\"training_data.csv\")\n",
    "df2=pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37be9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df1[\"X_train\"]\n",
    "y_train=df1[\"y_train\"]\n",
    "X_test=df2[\"X_test\"]\n",
    "y_test=df2[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c4c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting and transforming X_train using a Count Vectorizer with default parameters\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34425702",
   "metadata": {},
   "source": [
    "### 1) Multinomial Naive Bayes Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7adeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting a multinomial Naive Bayes Classifier Model with smoothing alpha=0.1\n",
    "model1 = sklearn.naive_bayes.MultinomialNB(alpha=0.1)\n",
    "model_fit = model1.fit(X_train_vectorized, y_train)\n",
    "predictions1 = model1.predict(vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ce53de",
   "metadata": {},
   "source": [
    "In evaluating the model’s performance, we can generate some predictions then look at the confusion matrix and AUC-ROC score to evaluate performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69c55dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted Spam  Predicted Ham\n",
      "Actual Spam            1181              6\n",
      "Actual Ham               11            196\n",
      "\n",
      "True Positives: 196\n",
      "False Positives: 6\n",
      "True Negatives: 1181\n",
      "False Negatives: 11\n",
      "True Positive Rate: 0.9468599033816425\n",
      "Specificity: 0.9949452401010952\n",
      "False Positive Rate: 0.005054759898904802\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions1).ravel()\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predictions1),\n",
    "             columns=['Predicted Spam', \"Predicted Ham\"], index=['Actual Spam', 'Actual Ham']))\n",
    "print(f'\\nTrue Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n",
    "print(f'True Negatives: {tn}')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "\n",
    "print(f'True Positive Rate: { (tp / (tp + fn))}')\n",
    "print(f'Specificity: { (tn / (tn + fp))}')\n",
    "print(f'False Positive Rate: { (fp / (fp + tn))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8848619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709025717413688"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making predictions & looking at AUC score\n",
    "aucscore = roc_auc_score(y_test, predictions1)\n",
    "aucscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f592742f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB Classifier</td>\n",
       "      <td>0.987805</td>\n",
       "      <td>0.970297</td>\n",
       "      <td>0.94686</td>\n",
       "      <td>0.958435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision   Recall  F1 Score\n",
       "0  NB Classifier  0.987805   0.970297  0.94686  0.958435"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = met.accuracy_score(y_test, predictions1)\n",
    "prec = met.precision_score(y_test, predictions1)\n",
    "rec = met.recall_score(y_test, predictions1)\n",
    "f1 = met.f1_score(y_test, predictions1)\n",
    "\n",
    "model1_results = pd.DataFrame([['NB Classifier', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "model1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1d56a4",
   "metadata": {},
   "source": [
    "### 2 Support Vector Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7ab1137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining an additional function\n",
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "352cb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit and transfor x_train and X_test\n",
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "'''The goal of using tfidf is to scale down the impact of tokens that occur very frequently in a given corpus and \n",
    "that are hence empirically less informative than features that occur in a small fraction of the training corpus.'''\n",
    "\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_train_transformed_with_length = add_feature(X_train_transformed, X_train.str.len())\n",
    "\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "X_test_transformed_with_length = add_feature(X_test_transformed, X_test.str.len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aac0143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM creation\n",
    "clf = SVC(C=10000)\n",
    "model2=clf.fit(X_train_transformed_with_length, y_train)\n",
    "predictions2 = clf.predict(X_test_transformed_with_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef5a84a",
   "metadata": {},
   "source": [
    "In evaluating the model’s performance, we can generate some predictions then look at the confusion matrix and AUC-ROC score to evaluate performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4abe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted Spam  Predicted Ham\n",
      "Actual Spam            1184              3\n",
      "Actual Ham               13            194\n",
      "\n",
      "True Positives: 194\n",
      "False Positives: 3\n",
      "True Negatives: 1184\n",
      "False Negatives: 13\n",
      "True Positive Rate: 0.9371980676328503\n",
      "Specificity: 0.9974726200505476\n",
      "False Positive Rate: 0.002527379949452401\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions2).ravel()\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predictions2),\n",
    "             columns=['Predicted Spam', \"Predicted Ham\"], index=['Actual Spam', 'Actual Ham']))\n",
    "print(f'\\nTrue Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n",
    "print(f'True Negatives: {tn}')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "\n",
    "print(f'True Positive Rate: { (tp / (tp + fn))}')\n",
    "print(f'Specificity: { (tn / (tn + fp))}')\n",
    "print(f'False Positive Rate: { (fp / (fp + tn))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "099a832c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673353438416991"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3912891d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>0.988522</td>\n",
       "      <td>0.984772</td>\n",
       "      <td>0.937198</td>\n",
       "      <td>0.960396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  Accuracy  Precision    Recall  F1 Score\n",
       "0  Support Vector Classifier  0.988522   0.984772  0.937198  0.960396"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = met.accuracy_score(y_test, predictions2)\n",
    "prec = met.precision_score(y_test, predictions2)\n",
    "rec = met.recall_score(y_test, predictions2)\n",
    "f1 = met.f1_score(y_test, predictions2)\n",
    "\n",
    "model2_results = pd.DataFrame([['Support Vector Classifier', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "model2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94210078",
   "metadata": {},
   "source": [
    "### 3 Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47f11cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = RandomForestClassifier(random_state = 1,  n_estimators = 200,\n",
    "                                    max_depth = 10, criterion = 'gini')\n",
    "model3.fit(X_train_vectorized, y_train)\n",
    "predictions3 = model3.predict(vect.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f581e7",
   "metadata": {},
   "source": [
    "In evaluating the model’s performance, we can generate some predictions then look at the confusion matrix and AUC-ROC score to evaluate performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38d6bb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Predicted Spam  Predicted Ham\n",
      "Actual Spam            1187              0\n",
      "Actual Ham              162             45\n",
      "\n",
      "True Positives: 45\n",
      "False Positives: 0\n",
      "True Negatives: 1187\n",
      "False Negatives: 162\n",
      "True Positive Rate: 0.21739130434782608\n",
      "Specificity: 1.0\n",
      "False Positive Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, predictions3).ravel()\n",
    "print(pd.DataFrame(confusion_matrix(y_test, predictions3),\n",
    "             columns=['Predicted Spam', \"Predicted Ham\"], index=['Actual Spam', 'Actual Ham']))\n",
    "print(f'\\nTrue Positives: {tp}')\n",
    "print(f'False Positives: {fp}')\n",
    "print(f'True Negatives: {tn}')\n",
    "print(f'False Negatives: {fn}')\n",
    "\n",
    "\n",
    "print(f'True Positive Rate: { (tp / (tp + fn))}')\n",
    "print(f'Specificity: { (tn / (tn + fp))}')\n",
    "print(f'False Positive Rate: { (fp / (fp + tn))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52b707c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6086956521739131"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, predictions3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12fdf40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest Gini (n=200)</td>\n",
       "      <td>0.883788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Accuracy  Precision    Recall  F1 Score\n",
       "0  Random Forest Gini (n=200)  0.883788        1.0  0.217391  0.357143"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = met.accuracy_score(y_test, predictions3)\n",
    "prec = met.precision_score(y_test, predictions3)\n",
    "rec = met.recall_score(y_test, predictions3)\n",
    "f1 = met.f1_score(y_test, predictions3)\n",
    "\n",
    "model_results3 = pd.DataFrame([['Random Forest Gini (n=200)', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "model_results3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d885d7",
   "metadata": {},
   "source": [
    "After training and testing these 3 models, it’s time to compare them. Comparing them based on AUC scores and Accuracy, we can see that The Naive Bayes Classifier had the highest scores, with the SVC model being marginally behind. "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
